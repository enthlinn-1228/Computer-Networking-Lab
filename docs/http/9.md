# Web 机器人
Web 机器人是能够在无需人类干预的情况下自动进行一系列 Web 事务处理的软件程序。很多机器人会从一个网站逛到另一个网站，获取内容，跟踪链接，并对它们找到的数据进行处理。

## 爬虫及爬行方式
爬虫是一种机器人，它们会递归地对各种信息网站进行遍历，获取第一个页面，然后获取那个页面指向的所有页面，依次类推。
搜索引擎使用爬虫在 Web 上游荡，并把它们碰到的文档全部拉回来。然后对这些文档进行处理，形成一个可搜索的数据库。

### 从哪儿开始：根集
爬虫开始访问的 URL 初始集合被称作根集。挑选根集时，应该从足够多不同的站点中选择 URL。

![](imgs/h9-1.png)

### 链接的提取和相对链接的标准化
爬虫在 Web 上移动时，会不停地对 HTML 页面进行解析。它对每个页面的 URL 链接进行分析，并将这些链接添加到需要爬行的页面列表中去。同时要将相对 URL 转换为绝对形式。

### 避免环路的出现
![](imgs/h9-2.png)

### 循环与复制
![](imgs/h9-3.png)

### 面包屑留下的痕迹
爬虫对访问过的地址进行管理时使用的一些技术：
1. 树和散列表。
2. 有损的存在位图，将爬行过的 URL 的存在位“置位”。
3. 检查点，一定要将已访问 URL 列表保存到硬盘上，以防机器人程序崩溃。
4. 分类，使用机器人集群来爬行整个 Web。

### 规范化 URL
![](imgs/h9-4.png)

大多数机器人都试图将 URL “规范化” 为标准格式来消除别名。规范化步骤：
1. 如果没有指定端口，就向主机名中添加“:80”。
2. 将所有转义字符 %xx 都转换成等价字符。
3. 删除 # 标签。

经过这些步骤就可以消除上图中 a~c 所列的别名问题了。

![](imgs/h9-5.png)

### 文件系统连接环路
![](imgs/h9-6.png)

### 避免循环和重复
有些技术可以使机器人避免环路的出现：
1. 规范化 URL。
2. 广度优先的爬行。
3. 节流，限制一段时间内机器人可以从一个网站获取的页面数量。
4. 限制 URL 的大小。
5. URL/站点黑名单，将会造成机器人环路的网站加入黑名单。
6. 模式检测，拒绝爬行多于两个或三个重复组件的 URL。
7. 内容指纹，对页面内容进行检验和，如果一个页面的校验和同之前某个检验和一样，就不去爬行这个页面。
8. 人工监视。

## 机器人的 HTTP
机器人和其他 HTTP 客户端程序没有什么区别，都要遵循 HTTP 规范中的规则。

## 行为不当的机器人
